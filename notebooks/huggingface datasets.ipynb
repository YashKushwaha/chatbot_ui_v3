{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32987556-8aff-48d8-b6e0-61bf06a39f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import get_dataset_config_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d5c576-aa21-4b7c-9f3d-e6a28c9837df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29886f30-7a6d-4143-9206-0ac76c55830d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"rajpurkar/squad_v2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6dbec3aa-e960-415b-9372-91dd9a1803ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = get_dataset_config_names(dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50ff7aa5-41d6-4399-a86e-f27cf3be42ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['squad_v2']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d5c1dfd-769f-4e01-a89b-97c9ff7c8f91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Array2D',\n",
       " 'Array3D',\n",
       " 'Array4D',\n",
       " 'Array5D',\n",
       " 'ArrowBasedBuilder',\n",
       " 'Audio',\n",
       " 'BuilderConfig',\n",
       " 'ClassLabel',\n",
       " 'Dataset',\n",
       " 'DatasetBuilder',\n",
       " 'DatasetDict',\n",
       " 'DatasetInfo',\n",
       " 'DownloadConfig',\n",
       " 'DownloadManager',\n",
       " 'DownloadMode',\n",
       " 'Features',\n",
       " 'GeneratorBasedBuilder',\n",
       " 'Image',\n",
       " 'IterableDataset',\n",
       " 'IterableDatasetDict',\n",
       " 'LargeList',\n",
       " 'NamedSplit',\n",
       " 'NamedSplitAll',\n",
       " 'Pdf',\n",
       " 'ReadInstruction',\n",
       " 'Sequence',\n",
       " 'Split',\n",
       " 'SplitBase',\n",
       " 'SplitDict',\n",
       " 'SplitGenerator',\n",
       " 'SplitInfo',\n",
       " 'StreamingDownloadManager',\n",
       " 'SubSplitInfo',\n",
       " 'Translation',\n",
       " 'TranslationVariableLanguages',\n",
       " 'Value',\n",
       " 'VerificationMode',\n",
       " 'Version',\n",
       " 'Video',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " '__version__',\n",
       " 'are_progress_bars_disabled',\n",
       " 'arrow_dataset',\n",
       " 'arrow_reader',\n",
       " 'arrow_writer',\n",
       " 'builder',\n",
       " 'combine',\n",
       " 'concatenate_datasets',\n",
       " 'config',\n",
       " 'data_files',\n",
       " 'dataset_dict',\n",
       " 'disable_caching',\n",
       " 'disable_progress_bar',\n",
       " 'disable_progress_bars',\n",
       " 'doc_utils',\n",
       " 'download',\n",
       " 'enable_caching',\n",
       " 'enable_progress_bar',\n",
       " 'enable_progress_bars',\n",
       " 'exceptions',\n",
       " 'experimental',\n",
       " 'extract',\n",
       " 'features',\n",
       " 'file_utils',\n",
       " 'filesystems',\n",
       " 'fingerprint',\n",
       " 'formatting',\n",
       " 'get_dataset_config_info',\n",
       " 'get_dataset_config_names',\n",
       " 'get_dataset_default_config_name',\n",
       " 'get_dataset_infos',\n",
       " 'get_dataset_split_names',\n",
       " 'hub',\n",
       " 'info',\n",
       " 'info_utils',\n",
       " 'inspect',\n",
       " 'interleave_datasets',\n",
       " 'is_caching_enabled',\n",
       " 'is_progress_bar_enabled',\n",
       " 'iterable_dataset',\n",
       " 'keyhash',\n",
       " 'load',\n",
       " 'load_dataset',\n",
       " 'load_dataset_builder',\n",
       " 'load_from_disk',\n",
       " 'logging',\n",
       " 'metadata',\n",
       " 'naming',\n",
       " 'packaged_modules',\n",
       " 'parallel',\n",
       " 'patching',\n",
       " 'percent',\n",
       " 'py_utils',\n",
       " 'search',\n",
       " 'sharding',\n",
       " 'splits',\n",
       " 'stratify',\n",
       " 'streaming',\n",
       " 'table',\n",
       " 'tf_utils',\n",
       " 'tqdm',\n",
       " 'track',\n",
       " 'typing',\n",
       " 'utils',\n",
       " 'version']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datasets as d\n",
    "dir(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8907a58-d7d2-4783-850e-111a879024fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function get_dataset_config_info in module datasets.inspect:\n",
      "\n",
      "get_dataset_config_info(path: str, config_name: Optional[str] = None, data_files: Union[str, collections.abc.Sequence[str], collections.abc.Mapping[str, Union[str, collections.abc.Sequence[str]]], NoneType] = None, download_config: Optional[datasets.download.download_config.DownloadConfig] = None, download_mode: Union[datasets.download.download_manager.DownloadMode, str, NoneType] = None, revision: Union[str, datasets.utils.version.Version, NoneType] = None, token: Union[bool, str, NoneType] = None, **config_kwargs) -> datasets.info.DatasetInfo\n",
      "    Get the meta information (DatasetInfo) about a dataset for a particular config\n",
      "    \n",
      "    Args:\n",
      "        path (``str``): path to the dataset processing script with the dataset builder. Can be either:\n",
      "    \n",
      "            - a local path to processing script or the directory containing the script (if the script has the same name as the directory),\n",
      "                e.g. ``'./dataset/squad'`` or ``'./dataset/squad/squad.py'``\n",
      "            - a dataset identifier on the Hugging Face Hub (list all available datasets and ids with [`huggingface_hub.list_datasets`]),\n",
      "                e.g. ``'rajpurkar/squad'``, ``'nyu-mll/glue'`` or ``'openai/webtext'``\n",
      "        config_name (:obj:`str`, optional): Defining the name of the dataset configuration.\n",
      "        data_files (:obj:`str` or :obj:`Sequence` or :obj:`Mapping`, optional): Path(s) to source data file(s).\n",
      "        download_config (:class:`~download.DownloadConfig`, optional): Specific download configuration parameters.\n",
      "        download_mode (:class:`DownloadMode` or :obj:`str`, default ``REUSE_DATASET_IF_EXISTS``): Download/generate mode.\n",
      "        revision (:class:`~utils.Version` or :obj:`str`, optional): Version of the dataset script to load.\n",
      "            As datasets have their own git repository on the Datasets Hub, the default version \"main\" corresponds to their \"main\" branch.\n",
      "            You can specify a different version than the default \"main\" by using a commit SHA or a git tag of the dataset repository.\n",
      "        token (``str`` or :obj:`bool`, optional): Optional string or boolean to use as Bearer token for remote files on the Datasets Hub.\n",
      "            If True, or not specified, will get token from `\"~/.huggingface\"`.\n",
      "        **config_kwargs (additional keyword arguments): optional attributes for builder class which will override the attributes if supplied.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(d.get_dataset_config_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8714350d-c0a4-40d3-a0c8-efe9dc7e00f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'description': '',\n",
       " 'citation': '',\n",
       " 'homepage': '',\n",
       " 'license': '',\n",
       " 'features': {'id': Value(dtype='string', id=None),\n",
       "  'title': Value(dtype='string', id=None),\n",
       "  'context': Value(dtype='string', id=None),\n",
       "  'question': Value(dtype='string', id=None),\n",
       "  'answers': Sequence(feature={'text': Value(dtype='string', id=None), 'answer_start': Value(dtype='int32', id=None)}, length=-1, id=None)},\n",
       " 'post_processed': None,\n",
       " 'supervised_keys': None,\n",
       " 'builder_name': 'parquet',\n",
       " 'dataset_name': 'squad_v2',\n",
       " 'config_name': 'squad_v2',\n",
       " 'version': 0.0.0,\n",
       " 'splits': {'train': SplitInfo(name='train', num_bytes=116732025, num_examples=130319, shard_lengths=None, dataset_name=None),\n",
       "  'validation': SplitInfo(name='validation', num_bytes=11661091, num_examples=11873, shard_lengths=None, dataset_name=None)},\n",
       " 'download_checksums': None,\n",
       " 'download_size': 17720493,\n",
       " 'post_processing_size': None,\n",
       " 'dataset_size': 128393116,\n",
       " 'size_in_bytes': None}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.get_dataset_config_info(dataset_name).__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "93112f85-eb96-4489-bc9e-142930b14549",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a2584202-468e-4d2d-836b-00b486d71006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function load_dataset in module datasets.load:\n",
      "\n",
      "load_dataset(path: str, name: Optional[str] = None, data_dir: Optional[str] = None, data_files: Union[str, collections.abc.Sequence[str], collections.abc.Mapping[str, Union[str, collections.abc.Sequence[str]]], NoneType] = None, split: Union[str, datasets.splits.Split, NoneType] = None, cache_dir: Optional[str] = None, features: Optional[datasets.features.features.Features] = None, download_config: Optional[datasets.download.download_config.DownloadConfig] = None, download_mode: Union[datasets.download.download_manager.DownloadMode, str, NoneType] = None, verification_mode: Union[datasets.utils.info_utils.VerificationMode, str, NoneType] = None, keep_in_memory: Optional[bool] = None, save_infos: bool = False, revision: Union[str, datasets.utils.version.Version, NoneType] = None, token: Union[bool, str, NoneType] = None, streaming: bool = False, num_proc: Optional[int] = None, storage_options: Optional[dict] = None, trust_remote_code: Optional[bool] = None, **config_kwargs) -> Union[datasets.dataset_dict.DatasetDict, datasets.arrow_dataset.Dataset, datasets.dataset_dict.IterableDatasetDict, datasets.iterable_dataset.IterableDataset]\n",
      "    Load a dataset from the Hugging Face Hub, or a local dataset.\n",
      "    \n",
      "    You can find the list of datasets on the [Hub](https://huggingface.co/datasets) or with [`huggingface_hub.list_datasets`].\n",
      "    \n",
      "    A dataset is a directory that contains some data files in generic formats (JSON, CSV, Parquet, etc.) and possibly\n",
      "    in a generic structure (Webdataset, ImageFolder, AudioFolder, VideoFolder, etc.)\n",
      "    \n",
      "    This function does the following under the hood:\n",
      "    \n",
      "        1. Load a dataset builder:\n",
      "    \n",
      "            * Find the most common data format in the dataset and pick its associated builder (JSON, CSV, Parquet, Webdataset, ImageFolder, AudioFolder, etc.)\n",
      "            * Find which file goes into which split (e.g. train/test) based on file and directory names or on the YAML configuration\n",
      "            * It is also possible to specify `data_files` manually, and which dataset builder to use (e.g. \"parquet\").\n",
      "    \n",
      "        2. Run the dataset builder:\n",
      "    \n",
      "            In the general case:\n",
      "    \n",
      "            * Download the data files from the dataset if they are not already available locally or cached.\n",
      "            * Process and cache the dataset in typed Arrow tables for caching.\n",
      "    \n",
      "                Arrow table are arbitrarily long, typed tables which can store nested objects and be mapped to numpy/pandas/python generic types.\n",
      "                They can be directly accessed from disk, loaded in RAM or even streamed over the web.\n",
      "    \n",
      "            In the streaming case:\n",
      "    \n",
      "            * Don't download or cache anything. Instead, the dataset is lazily loaded and will be streamed on-the-fly when iterating on it.\n",
      "    \n",
      "        3. Return a dataset built from the requested splits in `split` (default: all).\n",
      "    \n",
      "    It can also use a custom dataset builder if the dataset contains a dataset script, but this feature is mostly for backward compatibility.\n",
      "    In this case the dataset script file must be named after the dataset repository or directory and end with \".py\".\n",
      "    \n",
      "    Args:\n",
      "    \n",
      "        path (`str`):\n",
      "            Path or name of the dataset.\n",
      "    \n",
      "            - if `path` is a dataset repository on the HF hub (list all available datasets with [`huggingface_hub.list_datasets`])\n",
      "              -> load the dataset from supported files in the repository (csv, json, parquet, etc.)\n",
      "              e.g. `'username/dataset_name'`, a dataset repository on the HF hub containing the data files.\n",
      "    \n",
      "            - if `path` is a local directory\n",
      "              -> load the dataset from supported files in the directory (csv, json, parquet, etc.)\n",
      "              e.g. `'./path/to/directory/with/my/csv/data'`.\n",
      "    \n",
      "            - if `path` is the name of a dataset builder and `data_files` or `data_dir` is specified\n",
      "              (available builders are \"json\", \"csv\", \"parquet\", \"arrow\", \"text\", \"xml\", \"webdataset\", \"imagefolder\", \"audiofolder\", \"videofolder\")\n",
      "              -> load the dataset from the files in `data_files` or `data_dir`\n",
      "              e.g. `'parquet'`.\n",
      "    \n",
      "            It can also point to a local dataset script but this is not recommended.\n",
      "        name (`str`, *optional*):\n",
      "            Defining the name of the dataset configuration.\n",
      "        data_dir (`str`, *optional*):\n",
      "            Defining the `data_dir` of the dataset configuration. If specified for the generic builders (csv, text etc.) or the Hub datasets and `data_files` is `None`,\n",
      "            the behavior is equal to passing `os.path.join(data_dir, **)` as `data_files` to reference all the files in a directory.\n",
      "        data_files (`str` or `Sequence` or `Mapping`, *optional*):\n",
      "            Path(s) to source data file(s).\n",
      "        split (`Split` or `str`):\n",
      "            Which split of the data to load.\n",
      "            If `None`, will return a `dict` with all splits (typically `datasets.Split.TRAIN` and `datasets.Split.TEST`).\n",
      "            If given, will return a single Dataset.\n",
      "            Splits can be combined and specified like in tensorflow-datasets.\n",
      "        cache_dir (`str`, *optional*):\n",
      "            Directory to read/write data. Defaults to `\"~/.cache/huggingface/datasets\"`.\n",
      "        features (`Features`, *optional*):\n",
      "            Set the features type to use for this dataset.\n",
      "        download_config ([`DownloadConfig`], *optional*):\n",
      "            Specific download configuration parameters.\n",
      "        download_mode ([`DownloadMode`] or `str`, defaults to `REUSE_DATASET_IF_EXISTS`):\n",
      "            Download/generate mode.\n",
      "        verification_mode ([`VerificationMode`] or `str`, defaults to `BASIC_CHECKS`):\n",
      "            Verification mode determining the checks to run on the downloaded/processed dataset information (checksums/size/splits/...).\n",
      "    \n",
      "            <Added version=\"2.9.1\"/>\n",
      "        keep_in_memory (`bool`, defaults to `None`):\n",
      "            Whether to copy the dataset in-memory. If `None`, the dataset\n",
      "            will not be copied in-memory unless explicitly enabled by setting `datasets.config.IN_MEMORY_MAX_SIZE` to\n",
      "            nonzero. See more details in the [improve performance](../cache#improve-performance) section.\n",
      "        save_infos (`bool`, defaults to `False`):\n",
      "            Save the dataset information (checksums/size/splits/...).\n",
      "        revision ([`Version`] or `str`, *optional*):\n",
      "            Version of the dataset script to load.\n",
      "            As datasets have their own git repository on the Datasets Hub, the default version \"main\" corresponds to their \"main\" branch.\n",
      "            You can specify a different version than the default \"main\" by using a commit SHA or a git tag of the dataset repository.\n",
      "        token (`str` or `bool`, *optional*):\n",
      "            Optional string or boolean to use as Bearer token for remote files on the Datasets Hub.\n",
      "            If `True`, or not specified, will get token from `\"~/.huggingface\"`.\n",
      "        streaming (`bool`, defaults to `False`):\n",
      "            If set to `True`, don't download the data files. Instead, it streams the data progressively while\n",
      "            iterating on the dataset. An [`IterableDataset`] or [`IterableDatasetDict`] is returned instead in this case.\n",
      "    \n",
      "            Note that streaming works for datasets that use data formats that support being iterated over like txt, csv, jsonl for example.\n",
      "            Json files may be downloaded completely. Also streaming from remote zip or gzip files is supported but other compressed formats\n",
      "            like rar and xz are not yet supported. The tgz format doesn't allow streaming.\n",
      "        num_proc (`int`, *optional*, defaults to `None`):\n",
      "            Number of processes when downloading and generating the dataset locally.\n",
      "            Multiprocessing is disabled by default.\n",
      "    \n",
      "            <Added version=\"2.7.0\"/>\n",
      "        storage_options (`dict`, *optional*, defaults to `None`):\n",
      "            **Experimental**. Key/value pairs to be passed on to the dataset file-system backend, if any.\n",
      "    \n",
      "            <Added version=\"2.11.0\"/>\n",
      "        trust_remote_code (`bool`, *optional*, defaults to `None`):\n",
      "            Whether or not to allow for datasets defined on the Hub using a dataset script. This option\n",
      "            should only be set to `True` for repositories you trust and in which you have read the code, as it will\n",
      "            execute code present on the Hub on your local machine.\n",
      "    \n",
      "            <Added version=\"2.16.0\"/>\n",
      "    \n",
      "            <Changed version=\"2.20.0\">\n",
      "    \n",
      "            `trust_remote_code` defaults to `False` if not specified.\n",
      "    \n",
      "            </Changed>\n",
      "    \n",
      "        **config_kwargs (additional keyword arguments):\n",
      "            Keyword arguments to be passed to the `BuilderConfig`\n",
      "            and used in the [`DatasetBuilder`].\n",
      "    \n",
      "    Returns:\n",
      "        [`Dataset`] or [`DatasetDict`]:\n",
      "        - if `split` is not `None`: the dataset requested,\n",
      "        - if `split` is `None`, a [`~datasets.DatasetDict`] with each split.\n",
      "    \n",
      "        or [`IterableDataset`] or [`IterableDatasetDict`]: if `streaming=True`\n",
      "    \n",
      "        - if `split` is not `None`, the dataset is requested\n",
      "        - if `split` is `None`, a [`~datasets.streaming.IterableDatasetDict`] with each split.\n",
      "    \n",
      "    Example:\n",
      "    \n",
      "    Load a dataset from the Hugging Face Hub:\n",
      "    \n",
      "    ```py\n",
      "    >>> from datasets import load_dataset\n",
      "    >>> ds = load_dataset('cornell-movie-review-data/rotten_tomatoes', split='train')\n",
      "    \n",
      "    # Load a subset or dataset configuration (here 'sst2')\n",
      "    >>> from datasets import load_dataset\n",
      "    >>> ds = load_dataset('nyu-mll/glue', 'sst2', split='train')\n",
      "    \n",
      "    # Manual mapping of data files to splits\n",
      "    >>> data_files = {'train': 'train.csv', 'test': 'test.csv'}\n",
      "    >>> ds = load_dataset('namespace/your_dataset_name', data_files=data_files)\n",
      "    \n",
      "    # Manual selection of a directory to load\n",
      "    >>> ds = load_dataset('namespace/your_dataset_name', data_dir='folder_name')\n",
      "    ```\n",
      "    \n",
      "    Load a local dataset:\n",
      "    \n",
      "    ```py\n",
      "    # Load a CSV file\n",
      "    >>> from datasets import load_dataset\n",
      "    >>> ds = load_dataset('csv', data_files='path/to/local/my_dataset.csv')\n",
      "    \n",
      "    # Load a JSON file\n",
      "    >>> from datasets import load_dataset\n",
      "    >>> ds = load_dataset('json', data_files='path/to/local/my_dataset.json')\n",
      "    \n",
      "    # Load from a local loading script (not recommended)\n",
      "    >>> from datasets import load_dataset\n",
      "    >>> ds = load_dataset('path/to/local/loading_script/loading_script.py', split='train')\n",
      "    ```\n",
      "    \n",
      "    Load an [`~datasets.IterableDataset`]:\n",
      "    \n",
      "    ```py\n",
      "    >>> from datasets import load_dataset\n",
      "    >>> ds = load_dataset('cornell-movie-review-data/rotten_tomatoes', split='train', streaming=True)\n",
      "    ```\n",
      "    \n",
      "    Load an image dataset with the `ImageFolder` dataset builder:\n",
      "    \n",
      "    ```py\n",
      "    >>> from datasets import load_dataset\n",
      "    >>> ds = load_dataset('imagefolder', data_dir='/path/to/images', split='train')\n",
      "    ```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(load_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2c29b3d4-6889-4968-9f30-8555b33ebb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "client = MongoClient(\"mongodb://localhost:27017/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "21b55c0d-43a2-47ba-afb5-4247b173b49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = client.admin.command(\"ping\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f95ba5cc-6f7b-4678-aac3-92850cf7e610",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ok': 1.0}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0daddeb9-2de7-4c82-916d-6e6371d39b99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['HOST',\n",
       " 'PORT',\n",
       " '__annotations__',\n",
       " '__class__',\n",
       " '__class_getitem__',\n",
       " '__del__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__enter__',\n",
       " '__eq__',\n",
       " '__exit__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__iter__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__next__',\n",
       " '__orig_bases__',\n",
       " '__parameters__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__slots__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_after_fork',\n",
       " '_checkout',\n",
       " '_cleanup_cursor_lock',\n",
       " '_cleanup_cursor_no_lock',\n",
       " '_clients',\n",
       " '_close_cursor_now',\n",
       " '_close_cursor_soon',\n",
       " '_closed',\n",
       " '_codec_options',\n",
       " '_conn_for_reads',\n",
       " '_conn_for_writes',\n",
       " '_conn_from_server',\n",
       " '_connect',\n",
       " '_constructor_args',\n",
       " '_database_default_options',\n",
       " '_default_database_name',\n",
       " '_duplicate',\n",
       " '_encrypter',\n",
       " '_end_sessions',\n",
       " '_ensure_session',\n",
       " '_event_listeners',\n",
       " '_get_topology',\n",
       " '_host',\n",
       " '_init_background',\n",
       " '_init_based_on_options',\n",
       " '_init_kwargs',\n",
       " '_is_protocol',\n",
       " '_kill_cursor_impl',\n",
       " '_kill_cursors',\n",
       " '_kill_cursors_executor',\n",
       " '_kill_cursors_queue',\n",
       " '_list_databases',\n",
       " '_lock',\n",
       " '_loop',\n",
       " '_normalize_and_validate_options',\n",
       " '_opened',\n",
       " '_options',\n",
       " '_port',\n",
       " '_process_kill_cursors',\n",
       " '_process_periodic_tasks',\n",
       " '_process_response',\n",
       " '_read_concern',\n",
       " '_read_preference',\n",
       " '_read_preference_for',\n",
       " '_repr_helper',\n",
       " '_resolve_srv',\n",
       " '_resolve_srv_info',\n",
       " '_retry_internal',\n",
       " '_retry_with_session',\n",
       " '_retryable_read',\n",
       " '_retryable_write',\n",
       " '_return_server_session',\n",
       " '_run_operation',\n",
       " '_seeds',\n",
       " '_select_server',\n",
       " '_send_cluster_time',\n",
       " '_server_property',\n",
       " '_should_pin_cursor',\n",
       " '_start_session',\n",
       " '_timeout',\n",
       " '_tmp_session',\n",
       " '_topology',\n",
       " '_topology_settings',\n",
       " '_validate_kwargs_and_update_opts',\n",
       " '_write_concern',\n",
       " '_write_concern_for',\n",
       " 'address',\n",
       " 'arbiters',\n",
       " 'bulk_write',\n",
       " 'close',\n",
       " 'codec_options',\n",
       " 'drop_database',\n",
       " 'eq_props',\n",
       " 'get_database',\n",
       " 'get_default_database',\n",
       " 'is_mongos',\n",
       " 'is_primary',\n",
       " 'list_database_names',\n",
       " 'list_databases',\n",
       " 'next',\n",
       " 'nodes',\n",
       " 'options',\n",
       " 'primary',\n",
       " 'read_concern',\n",
       " 'read_preference',\n",
       " 'secondaries',\n",
       " 'server_info',\n",
       " 'start_session',\n",
       " 'topology_description',\n",
       " 'watch',\n",
       " 'write_concern']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(client)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
